pub_date	title	venue	authors	url_slug	paper_url	abstract
2017/06/29	SVD-based Adaptive QIM Watermarking on Stereo Audio Signals	IEEE Transactions on Multimedia	"**<U>Min-Jae Hwang</U>**, JeeSok Lee, MiSuk Lee, and Hong-Goo Kang"	2017-1-svd	https://ieeexplore.ieee.org/abstract/document/7962215	"This paper proposes a blind digital audio water- marking algorithm that utilizes the quantization index modulation (QIM) and the singular value decomposition (SVD) of stereo audio signals. Conventional SVD-based blind audio watermarking algorithms lack physical interpretation since the matrix construction method for the input matrix for SVD is heuristically defined. However, in the proposed approach, because the SVD is directly applied to the stereo input signals, the resulting decomposed elements convey a conceptually meaningful inter- pretation of the original audio signal. As the proposed approach effectively utilizes the ratio of singular values, the embedded watermark is highly imperceptible and robust against volumetric scaling attacks; most QIM-based watermarking schemes are weak to these types of attacks. Experimental results under well-known practical attacks, such as compressions, resampling, and various types of signal processing, confirm that the proposed algorithm performs well compared to conventional audio watermarking algorithms."
2018/04/05	Modeling-by-Generation-structured Noise Compensation Algorithm for Glottal Vocoding Speech Synthesis System	ICASSP 2018	"**<U>Min-Jae Hwang</U>**, Eunwoo Song, Kyungguen Byun, and Hong-Goo Kang"	2018-1-mbg	https://sewplay.github.io/cv/papers/2018/icassp_0005669.pdf	"This paper proposes a novel noise compensation algorithm for a glottal excitation model in a deep learning (DL)-based speech synthesis system. To generate high-quality speech synthesis outputs, the balance between harmonic and noise components of the glottal excitation signal should be wellrepresented by the DL network. However, it is hard to accurately model the noise component because the DL training process inevitably results in statistically smoothed outputs; thus, it is essential to introduce an additional noise compensation process. We propose a modeling-by-generation structurebased noise compensation method that the missing noise component in the generated glottal signal is directly extracted and parameterized during the entire training process. By modeling the noise component using the additional DL network, the proposed system successfully compensates the missing noise component. Objective and subjective test results confirm that the synthesized speech with the proposed noise compensation method is superior to that with conventional methods."
2018/09/02	A Unified Framework for the Generation of Glottal Signals in Deep Learning-based Parametric Speech Synthesis Systems	Interspeech 2018	"**<U>Min-Jae Hwang</U>**, Eunwoo Song, Jin-Seob Kim, and Hong-Goo Kang"	2018-2-unified-mbg	https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1590.pdf	"In this paper, we propose a unified training framework for the  generation of glottal signals in deep learning (DL)-based parametric speech synthesis systems. The glottal vocoding-based speech synthesis system, especially the  odeling-by-generation (MbG) structure that we proposed recently, significantly improves the naturalness of synthesized speech by faithfully representing the noise component of the glottal excitation with an additional DL structure.  ecause the MbG method introduces a multistage processing pipeline, however, its training process is complicated and inefficient. To alleviate this problem, we propose a unified training approach that directly generates speech parameters by merging all the required models, such as acoustic, glottal, and noise models, into a single unified network. Considering the fact that noise analysis should be performed after training the glottal model, we also propose    tochastic noise analysis method that enables noise modeling to be included in the unified training process by iteratively analyzing the noise component in every epoch. Both objective and subjective test results verify the superiority of the proposed algorithm compared to conventional methods."
2019/06/20	Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment	Interspeech 2019	**<U>Min-Jae Hwang</U>** and Hong-Goo Kang	2019-1-param-enhance	https://arxiv.org/abs/1906.08407	"In this paper, we propose a deep learning (DL)-based parameter enhancement method for a mixed excitation linear prediction (MELP) speech codec in noisy communication environment. Unlike conventional speech enhancement modules that are designed to obtain clean speech signal by removing noise components before speech codec processing, the proposed method directly enhances codec parameters on either the encoder or decoder side. As the proposed method has been implemented by a small network without any additional processes required in conventional enhancement systems, e.g., time-frequency (T-F) analysis/synthesis modules, its computational complexity is very low. By enhancing the noise-corrupted codec parameters with the proposed DL framework, we achieved an enhancement system that is much simpler and faster than conventional T-F mask-based speech enhancement methods, while the quality of its performance remains similar."
2020/05/04	Improving LPCNet-based Text-to-Speech with Linear Prediction-structured Mixture Density Network	ICASSP 2020	"**<U>Min-Jae Hwang</U>**, Eunwoo Song, Ryuichi Yamamoto, Frank Soong, Hong-Goo Kang"	2020-1-ilpcnet	https://ieeexplore.ieee.org/abstract/document/9053704	"In this paper, we propose an improved LPCNet vocoder using a linear prediction (LP)-structured mixture density network (MDN). The recently proposed LPCNet vocoder has successfully achieved high-quality and lightweight speech synthesis systems by combining a vocal tract LP filter with a WaveRNN-based vocal source (i.e., excitation) generator. However, the quality of synthesized speech is often unstable because the vocal source component is insufficiently represented by the _-law quantization method, and the model is trained without considering the entire speech production mechanism. To address this problem, we first introduce LP-MDN, which enables the autoregressive neural vocoder to structurally represent the interactions between the vocal tract and vocal source components. Then, we propose to incorporate the LP-MDN to the LPCNet vocoder by replacing the conventional discretized output with continuous density distribution. The experimental results verify that the proposed system provides high quality synthetic speech by achieving a mean opinion score of 4.41 within a text-to-speech framework."
2020/08/01	Neural Text-to-Speech with a Modeling-by-Generation Excitation Vocoder	INTERSPEECH 2020	"Eunwoo Song, **<U>Min-Jae Hwang</U>**, Ryuichi Yamamoto, Jin-Seob Kim, Ohsung Kwon, Jae-Min Kim"	2020-2-mbg-excitnet	https://arxiv.org/abs/2008.00132	"This paper proposes a modeling-by-generation (MbG) excitation vocoder for a neural text-to-speech (TTS) system. Recently proposed neural excitation vocoders can realize qualified waveform generation by combining a vocal tract filter with a WaveNet-based glottal excitation generator. However, when these vocoders are used in a TTS system, the quality of synthesized speech is often degraded owing to a mismatch between training and synthesis steps. Specifically, the vocoder is separately trained from an acoustic model front-end. Therefore, estimation errors of the acoustic model are inevitably boosted throughout the synthesis process of the vocoder back-end. To address this problem, we propose to incorporate an MbG structure into the vocoder's training process. In the proposed method, the excitation signal is extracted by the acoustic model's generated spectral parameters, and the neural vocoder is then optimized not only to learn the target excitation's distribution but also to compensate for the estimation errors occurring from the acoustic model. Furthermore, as the generated spectral parameters are shared in the training and synthesis steps, their mismatch conditions can be reduced effectively. The experimental results verify that the proposed system provides high-quality synthetic speech by achieving a mean opinion score of 4.57 within the TTS framework."
2020/12/07	LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis	APSIPA ASC 2020	"**<U>Min-Jae Hwang</U>**, Frank Soong, Eunwoo Song, Xi Wang, Hyeonjoo Kang, Hong-Goo Kang"	2020-3-lpwavenet	https://arxiv.org/abs/1811.11913	"We propose a linear prediction (LP)-based waveform generation method via WaveNet vocoding framework. A WaveNet-based neural vocoder has significantly improved the quality of parametric text-to-speech (TTS) systems. However, it is challenging to effectively train the neural vocoder when the target database contains massive amount of acoustical information such as prosody, style or expressiveness. As a solution, the approaches that only generate the vocal source component by a neural vocoder have been proposed. However, they tend to generate synthetic noise because the vocal source component is independently handled without considering the entire speech production process; where it is inevitable to come up with a mismatch between vocal source and vocal tract filter. To address this problem, we propose an LP-WaveNet vocoder, where the complicated interactions between vocal source and vocal tract components are jointly trained within a mixture density network-based WaveNet model. The experimental results verify that the proposed system outperforms the conventional WaveNet vocoders both objectively and subjectively. In particular, the proposed method achieves 4.47 MOS within the TTS framework."
2020/12/07	ExcitGlow: Improving a WaveGlow-based Neural Vocoder with Linear Prediction Analysis	APSIPA ASC 2020	"Suhyeon Oh, Hyungseob Lim, Kyungguen Byun, **<U>Min-Jae Hwang</U>**, Eunwoo Song, Hong-Goo Kang"	2020-4-excitglow	https://sewplay.github.io/cv/papers/2020/apsipa_0000831.pdf	"In this paper, we propose ExcitGlow, a vocoder that incorporates the source-filter model of voice production theory into a flow-based deep generative model. By targeting the distribution of the excitation signal instead of the speech waveform itself, we significantly reduce the size of the flow-based generative model. To further reduce the number of parameters, we apply a parameter sharing technique in which a single affine coupling layer is used for several  low layers. To avoid quality degradation, we also introduce a closed-loop training framework to optimize the flow model for both the speech and excitation signal generation processes. Specifically, we choose negative log-likelihood (NLL) loss for the excitation signal and multi-resolution spectral distance for the speech signal. As a result, we are able to reduce the model size from 87.73M to 15.60M parameters while maintaining the perceptual quality of synthesized speech."
2021/01/19	Improved Parallel WaveGAN Vocoder with Perceptually Weighted Spectrogram Loss	IEEE SLT Workshop 2021	"Eunwoo Song, Ryuichi Yamamoto, **<U>Min-Jae Hwang</U>**, Jin-Seob Kim, Ohsung Kwon, Jae-Min Kim"	2021-1-pwg-perceptual-loss	https://arxiv.org/abs/2101.07412	"This paper proposes a spectral-domain perceptual weighting technique for Parallel WaveGAN-based text-to-speech (TTS) systems. The recently proposed Parallel WaveGAN vocoder successfully generates waveform sequences using a fast non-autoregressive WaveNet model. By employing multi-resolution short-time Fourier transform (MR-STFT) criteria with a generative adversarial network, the light-weight convolutional networks can be effectively trained without any distillation process. To further improve the vocoding performance, we propose the application of frequency-dependent weighting to the MR-STFT loss function. The proposed method penalizes perceptually-sensitive errors in the frequency domain; thus, the model is optimized toward reducing auditory noise in the synthesized speech. Subjective listening test results demonstrate that our proposed method achieves 4.21 and 4.26 TTS mean opinion scores for female and male Korean speakers, respectively."
2021/06/06	Parallel Waveform Synthesis based on Generative Adversarial Networks with Voicing-aware Conditional Discriminators	ICASSP 2021	"Ryuichi Yamamoto, Eunwoo Song, **<U>Min-Jae Hwang</U>**, Jae-Min Kim"	2021-2-pwg-voicing-disc	https://ieeexplore.ieee.org/abstract/document/9413369	"This paper proposes voicing-aware conditional discriminators for Parallel WaveGAN-based waveform synthesis systems. In this framework, we adopt a projection-based conditioning method that can significantly improve the discriminator’s performance. Furthermore, the conventional discriminator is separated into two waveform discriminators for modeling voiced and unvoiced speech. As each discriminator learns the distinctive characteristics of the harmonic and noise components, respectively, the adversarial training process becomes more efficient, allowing the generator to produce more realistic speech waveforms. Subjective test results demonstrate the superiority of the proposed method over the conventional Parallel WaveGAN and WaveNet systems. In particular, our speaker-independently trained model within a FastSpeech 2 based text-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21, and 4.31 for four Japanese speakers, respectively."
2021/06/06	TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-quality Speech Synthesis	ICASSP 2021	"**<U>Min-Jae Hwang</U>**, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim"	2021-3-txt	https://ieeexplore.ieee.org/abstract/document/9414408	"In this paper, we propose a text-to-speech (TTS)-driven data augmentation method for improving the quality of a non-autoregressive (AR) TTS system. Recently proposed non-AR models, such as FastSpeech 2, have successfully achieved fast speech synthesis system. However, their quality is not satisfactory, especially when the amount of training data is insufficient. To address this problem, we propose an effective data augmentation method using a well-designed AR TTS system. In this method, large-scale synthetic corpora including text-waveform pairs with phoneme duration are generated by the AR TTS system, and then used to train the target non-AR model. Perceptual listening test results showed that the proposed method significantly improved the quality of the non-AR TTS system. In particular, we augmented five hours of a training database to 179 hours of a synthetic one. Using these databases, our TTS system consisting of a FastSpeech 2 acoustic model with a Parallel WaveGAN vocoder achieved a mean opinion score of 3.74, which is 40% higher than that achieved by the conventional method."
2021/09/01	LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks	Interspeech 2021	"Huu-Kim Nguyen, Kihyuk Jeong, Seyun Um, **<U>Min-Jae Hwang</U>**, Eunwoo Song, Hong-Goo Kang"	2021-4-litetts	https://sewplay.github.io/cv/papers/2021/IS210188.pdf	"In this paper, we propose a lightweight end-to-end text-tospeech model that can generate high-quality speech at breakneck speed. In our proposed model, a feature prediction module and a waveform generation module are combined within a single framework. The feature prediction module, which consists of two independent sub-modules, estimates latent space embeddings for input text and prosodic information, and the waveform generation module generates speech waveforms by conditioning on the estimated latent space embeddings. Unlike conventional approaches that estimate prosodic information using a pre-trained model, our model jointly trains the prosodic embedding network with the speech waveform generation task using an effective domain transfer technique. Experimental results show that our proposed model can generate samples 7 times faster than real-time, and about 1.6 times faster than FastSpeech 2, as we use only 13.4 million parameters. We confirm that the generated speech quality is still of a high standard as evaluated by mean opinion scores."
2021/09/01	High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model	Interspeech 2021	"**<U>Min-Jae Hwang</U>**, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim"	2021-5-hnpwg	https://sewplay.github.io/cv/papers/2021/IS210976.pdf	"This paper proposes a multi-band harmonic-plus-noise (HN) Parallel WaveGAN (PWG) vocoder. To generate a highfidelity speech signal, it is important to well-reflect the
harmonic-noise characteristics of the speech waveform in the time-frequency domain. However, it is difficult for the conventional PWG model to accurately match this condition, as its single generator inefficiently represents the complicated nature of harmonic-noise structures. In the proposed method, the HN WaveNet models are employed to overcome this limitation, which enable the separate generation of the harmonic and noise components of speech signals from the pitch-dependent sine wave and Gaussian noise sources, respectively. Then, the energy ratios between harmonic and noise components in multiple frequency bands (i.e., subband harmonicities) are predicted by an additional harmonicity estimator. Weighted by the estimated harmonicities, the gain of harmonic and noise components in each subband is adjusted, and finally mixed together to compose the full-band speech signal. Subjective evaluation results showed that the proposed method significantly improved the perceptual quality of the synthesized speech."
2022/02/06	Effective Data Augmentation Methods for Neural Text-to-Speech Systems	ICEIC 2022	"Suhyeon Oh, Ohsung Kwon, **<U>Min-Jae Hwang</U>**, Jae-Min Kim, Eunwoo Song"	2022-1-rsvm-aug	https://ieeexplore.ieee.org/abstract/document/9748515	"This paper proposes an effective self-augmentation method for improving the quality of neural text-to-speech (TTS) systems. As synthetic speech quality has been greatly improved, creating a neural TTS system using synthetic corpora is now possible. However, whether increasing the amount of synthetic data is always beneficial for improving training efficiency has not been verified. Our aim in this study is to selectively choose synthetic data whose characteristics are close to those of natural speech. Specifically, we adopt a ranking support vector machine (RankSVM) that is well known for effectively ranking relative attributes among binary classes. By setting the synthetic and recorded corpora as two opposite classes, RankSVM is used to determine how the synthesized speech is acoustically similar with the recorded data. As training data can be selectively chosen from large-scale synthetic corpora, the performance of the TTS model re-trained by those data is significantly improved. Subjective evaluation results verify that the proposed TTS model performs much better than the original model trained with recorded data alone and the similarly configured system re-trained with all the synthetic data without any selection method."
2022/02/06	Linear Prediction-based Parallel WaveGAN Speech Synthesis	ICEIC 2022	"**<U>Min-Jae Hwang</U>**, Hyun-Wook Yoon, Chan-Ho Song, Jin-Seob Kim, Jae-Min Kim, Eunwoo Song"	2022-2-lphnpwg	https://ieeexplore.ieee.org/abstract/document/9748530/	"This paper proposes a linear prediction (LP)-based neural speech synthesis method for a Parallel WaveGAN (PWG) framework. A recently proposed PWG vocoder successfully generates waveform sequences using a fast non-autoregressive Wave Net model. However, it often suffers from noisy outputs because of difficulties in capturing the complicated time-varying nature of speech signals. To improve synthesis quality, we introduce a back-propagable LP synthesis method for a PWG framework. Based on a source-filter theory of speech production model, the proposed PWG model learns the behavior of a source excitation signal, which is decoupled from a speech signal using the LP synthesis filter. In this way, it is possible to separately train the characteristics of only excitation signal while considering the interaction between the vocal source and vocal tract filter. Thus, the quality of the synthesized speech signal can be further improved. Objective and subjective evaluation results verified that the proposed methods reconstruct significantly better quality of synthetic speech than conventional methods."
2022/06/30	TTS-by-TTS 2: Data-selective Augmentation for Neural Speech Synthesis using Ranking Support Vector Machine with Variational Autoencoder	Interspeech 2022	"Eunwoo Song, Ryuichi Yamamoto, Ohsung Kwon, Chan-Ho Song, **<U>Min-Jae Hwang</U>**, Suhyeon Oh, Hyun-Wook Yoon, Jin-Seob Kim, Jae-Min Kim"	2022-3-txt2	https://arxiv.org/abs/2206.14984	"Recent advances in synthetic speech quality have enabled us to train text-to-speech (TTS) systems by using synthetic corpora. However, merely increasing the amount of synthetic data is not always advantageous for improving training efficiency. Our aim in this study is to selectively choose synthetic data that are beneficial to the training process. In the proposed method, we first adopt a variational autoencoder whose posterior distribution is utilized to extract latent features representing acoustic similarity between the recorded and synthetic corpora. By using those learned features, we then train a ranking support vector machine (RankSVM) that is well known for effectively ranking relative attributes among binary classes. By setting the recorded and synthetic ones as two opposite classes, RankSVM is used to determine how the synthesized speech is acoustically similar to the recorded data. Then, synthetic TTS data, whose distribution is close to the recorded data, are selected from large-scale synthetic corpora. By using these data for retraining the TTS model, the synthetic quality can be significantly improved. Objective and subjective evaluation results show the superiority of the proposed method over the conventional methods."
2022/06/30	Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems	Interspeech 2022	"Hyun-Wook Yoon, Ohsung Kwon, Hoyeon Lee, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim, **<U>Min-Jae Hwang</U>**"	2022-4-lmtts	https://arxiv.org/abs/2206.15067	"This paper proposes an effective emotional text-to-speech (TTS) system with a pre-trained language model (LM)-based emotion prediction method. Unlike conventional systems that require auxiliary inputs such as manually defined emotion classes, our system directly estimates emotion-related attributes from the input text. Specifically, we utilize generative pre-trained transformer (GPT)-3 to jointly predict both an emotion class and its strength in representing emotions coarse and fine properties, respectively. Then, these attributes are combined in the emotional embedding space and used as conditional features of the TTS model for generating output speech signals. Consequently, the proposed system can produce emotional speech only from text without any auxiliary inputs. Furthermore, because the GPT-3 enables to capture emotional context among the consecutive sentences, the proposed method can effectively handle the paragraph-level generation of emotional speech."